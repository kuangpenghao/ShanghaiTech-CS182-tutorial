\item \defpoints{10} [Linear Classification] Consider the ``Multi-class Logistic Regression'' algorithm. Given training set $\mathcal{D}=\{(x^i,y^i)\mid i=1,\ldots,n\}$ where $x^i\in \mathbb{R}^{p+1}$ is the feature vector and $y^i\in \mathbb{R}^{k}$ is a one-hot binary vector indicating $k$ classes. We want to find the parameter $\hat{\beta}=[\hat{\beta}_1,\ldots,\hat{\beta}_k]\in \mathbb{R}^{(p+1)\times k}$ that maximize the likelihood for the training set. Introducing the softmax function, we assume our model has the form
$$p(y_c^i=1\mid x^i;\beta) = \frac{\exp(\beta_c^\top x^i)}{\sum_{c'}\exp(\beta_{c'}^\top x^i)}$$
where $y_c^i$ is the $c$-th element of $y^i$.

\begin{itemize}
\item[(a)] Complete the derivation of the conditional log likelihood for our model, which is
\begin{align*}
    \ell(\beta) = \ln \prod_{i=1}^{n} p(y_t^i\mid x^i;\beta)
    =\sum_{i=1}^{n}\sum_{c=1}^{k}\left[ y_c^i(\beta_c^\top x^i) - y_c^i\ln \left(\sum_{c'}\exp(\beta_{c'}^\top x^i) \right)\right].
\end{align*}
For simplicity, we abbreviate $p(y_t^i=1\mid x^i;\beta)$ as $p(y_t^i\mid x^i;\beta)$, where $t$ is the true class for $x^i$.~\defpoints{5}
\item[(b)] Derive the gradient of $\ell(\beta)$ w.r.t. $\beta_1$, i.e.,
$$\nabla_{\beta_1}\ell(\beta) = \nabla_{\beta_1} \sum_{i=1}^{n}\sum_{c=1}^{k}\left[ y_c^i(\beta_c^\top x^i) - y_c^i\ln \left(\sum_{c'}\exp(\beta_{c'}^\top x^i) \right)\right]$$

Remark: Log likelihood is always concave; thus, we can optimize our model using gradient ascent. (The gradient of $\ell(\beta)$ w.r.t. $\beta_2,\ldots,\beta_k$ is similar, you don't need to write them)~\defpoints{5}
\end{itemize}

\solution

(a) Since the model with softmax function is that $p(y_c^i=1|x^i;\beta)=\dfrac{\exp(\beta_c^{\top}x^i)}{\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)}$. \\
And since $y^i$ is a one-hot binary vector, so $y_t^i=1$, and $\forall c\neq t, y_{c}^i=0$. \\
So $\forall i\in\{1,2,\cdots,n\},\forall c\in\{1,2,\cdots,k\}$, we have
$$p(y^i|x^i;\beta)=p(y_t^i|x^i;\beta)=\prod\limits_{c=1}^k p(y_{c}^i|x^i;\beta)^{y_{c}^i}$$

So the likelihood is that
$$L(\beta)=\prod\limits_{i=1}^np(y^i|x^i;\beta)=\prod\limits_{i=1}^n\prod\limits_{c=1}^k p(y_c^i|x^i;\beta)^{y_c^i}$$
So the log-likelihood is that
$$\ell(\beta)=\log L(\beta)=\sum\limits_{i=1}^n\sum\limits_{c=1}^k y_c^i\log p(y_c^i|x^i;\beta)=\sum\limits_{i=1}^n\sum\limits_{c=1}^k y_c^i\log\dfrac{\exp(\beta_c^{\top}x^i)}{\sum\limits_{c'}exp(\beta_{c'}^{\top}x^i)}$$
$$=\sum\limits_{i=1}^n\sum\limits_{c=1}^k\left[y_c^i(\beta_c^{\top}x^i)-y_c^i\log\left(\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)\right)\right]$$

So above all, the log-likelihood is that
$$\ell(\beta)=\sum\limits_{i=1}^n\sum\limits_{c=1}^k\left[y_c^i(\beta_c^{\top}x^i)-y_c^i\log\left(\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)\right)\right]$$

(b) The gradient of $\ell(\beta)$ w.r.t. $\beta_1$ is that
$$\nabla_{\beta_1}\ell(\beta)=\nabla_{\beta_1}\sum\limits_{i=1}^n\sum\limits_{c=1}^k\left[y_c^i(\beta_c^{\top}x^i)-y_c^i\log\left(\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)\right)\right]$$
If $c\neq 1$, then $\nabla_{\beta_1}y_c^i\beta_c^{\top}x^i=0$, and if $c=1$, then $\nabla_{\beta_1}y_c^i\beta_c^{\top}x^i=y_c^ix^i=y_{1}^ix^i$. \\
And $\nabla_{\beta_1}\log\left(\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)\right)=\dfrac{\exp(\beta_1^{\top}x^i)x^i}{\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)}$. \\
So

$$\nabla_{\beta_1}\ell(\beta)=\sum\limits_{i=1}^n\left(\sum\limits_{c=1}^k\nabla_{\beta_1}y_c^i\beta_c^{\top}x^i-\sum\limits_{c=1}^k\nabla_{\beta_1}y_c^i\log\left(\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)\right)\right)$$
$$=\sum\limits_{i=1}^n\left(y_1^ix^i-\left(\sum\limits_{c=1}^ky_c^i\right)\cdot\dfrac{\exp(\beta_1^{\top}x^i)x^i}{\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)}\right)$$
Also, since $y^i$ is a one-hot binary vector, so $y_t^i=1$, and $\forall c\neq t, y_{c}^i=0$. \\
So $\sum\limits_{c=1}^ky_c^i=1$. \\
So
$$\nabla_{\beta_1}\ell(\beta)=\sum\limits_{i=1}^n\left(y_1^ix^i-\dfrac{\exp(\beta_1^{\top}x^i)x^i}{\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)}\right)$$
So above all
$$\nabla_{\beta_1}\ell(\beta)=\sum\limits_{i=1}^n\left(y_1^ix^i-\dfrac{\exp(\beta_1^{\top}x^i)x^i}{\sum\limits_{c'}\exp(\beta_{c'}^{\top}x^i)}\right)$$

\newpage