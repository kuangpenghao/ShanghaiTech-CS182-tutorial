\item \defpoints{20} [Maximum Margin Classifier]
Consider a data set of $n\ d$-dimensional sample points, $\left\{x_1, \ldots, x_n\right\}$. Each sample point, $x_i \in \mathbb{R}^d$, has a corresponding label, $y_i$, indicating to which class that point belongs. For now, we will assume that there are only two classes and that every point is either in the given class $\left(y_i=1\right)$ or not in the class $\left(y_i=-1\right)$. Consider the linear decision boundary defined by the hyperplane
$$
\mathcal{H}=\left\{x \in \mathbb{R}^d: x \cdot w+\alpha=0\right\} .
$$
The maximum margin classifier maximizes the distance from the linear decision boundary to the closest training point on either side of the boundary, while correctly classifying all training points. Suppose the points are linear seperable, and the margin is $\gamma$.

\begin{itemize}
\item[(a)]The maximum margin classifier aims to maximize the distance from the training points to the decision boundary. Derive the distance from a point $x_i$ to the hyperplane $\mathcal{H}$. ~\defpoints{5}

\item[(b)] An in-class sample point is correctly classified if it is on the positive side of the decision boundary, and an out-of-class sample is correctly classified if it is on the negative side. Assuming all the points are correctly classified, write a set of $n$ constraints to ensure that all $n$ points are correctly classified. ~\defpoints{5}

\item[(c)] Using the previous parts, write an optimization problem for the maximum margin classifier. For convinent, we should additionally add a constrain $\|w\|=1$ ~\defpoints{5}

\item[(d)] To simply the optimization problem, we can rewrite the optimization problem in part (c) by setting $w'=\dfrac{w}{\gamma}$ and $\alpha'=\dfrac{\alpha}{\gamma}$. Write the optimization problem for the simlified maximum margin classifier. ~\defpoints{5}

\end{itemize}

\solution

(a) For any point $x_i$, suppose that the projection of $x_i$ on the hyperplane $\mathcal{H}$ is $x$, then we have
$$x\cdot w+\alpha=0$$
And since $x$ is the projection of $x_i$ on the hyperplane $\mathcal{H}$, so we have $(x_i-x)\perp \mathcal{H}$, which means
$(x_i-x) \parallel w$. So we can suppose that $x_i-x=d \dfrac{w}{\|w\|}$, then we have
\begin{align*}
d \dfrac{w}{\|w\|}&=x_i - x \\
d \dfrac{w^{\top}w}{\|w\|}&=w^{\top}(x_i - x)  \text{ (multiply $w^{\top}$ on both sides)} \\
d \dfrac{\|w\|^2}{\|w\|}&=w^{\top}x_i - w^{\top}x = w^{\top}x_i + \alpha \text{ ($w^{\top}x+\alpha=0$)} \\
d &= \dfrac{w^{\top}x_i + \alpha}{\|w\|}
\end{align*}

And since $x_i$ could be in the positive side or negative side of the hyperplane $\mathcal{H}$, so $d$ may be positive or negetive.
So the distance from a point $x_i$ to the hyperplane $\mathcal{H}$ is
$$r = |d| = \dfrac{|w^{\top}x_i + \alpha|}{\|w\|}$$

So above all, the distance from a point $x_i$ to the hyperplane $\mathcal{H}$ is
$$r = \dfrac{|w^{\top}x_i + \alpha|}{\|w\|}$$

(b) Since all sample points are correctly classified, so for the in-class sample points, the label $y_i=1$ and it is on the positive side of the decision boundary, so we have $\dfrac{{x_i\cdot w+\alpha}}{\|w\|}\geq \gamma$.
So $\dfrac{y_i(x_i\cdot w+\alpha)}{\|w\|}\geq \gamma$.

For the out-of-class sample points, the label $y_i=-1$ and it is on the negative side of the decision boundary, so we have $\dfrac{-(x_i\cdot w+\alpha)}{\|w\|}\geq \gamma$.
So $\dfrac{y_i(x_i\cdot w+\alpha)}{\|w\|}\geq \gamma$.

So above all, we have the constraints as follows:
$$\dfrac{y_i(x_i\cdot w+\alpha)}{\|w\|}\geq \gamma, \forall i\in\{1,2,\cdots,n\}$$

It is also acceptible is you write
$$\dfrac{y_i(x_i\cdot w+\alpha)}{\|w\|}> 0, \forall i\in\{1,2,\cdots,n\}$$
or
$$y_i(x_i\cdot w+\alpha)> 0, \forall i\in\{1,2,\cdots,n\}$$

(c) The original problem for the maximum margin classifier is
\begin{equation}
\begin{aligned}
&\max_{w,\alpha, \gamma}\qquad\quad \gamma \\
&\text{subject to}\quad \|w\|=1 \\
&\qquad\qquad\quad\ y_i(x_i\cdot w+\alpha)\geq \gamma,\ \forall i\in\{1,2,\cdots,n\} \\
\end{aligned}
\end{equation}

(d) Let $w'=\dfrac{w}{\gamma}$ and $\alpha'=\dfrac{\alpha}{\gamma}$. And since $\gamma=\dfrac{\|w\|}{\|w'\|}=\dfrac{1}{\|w'\|}$, so maximize $\gamma$ is equivalent to minimize $\|w'\|=\dfrac{1}{\gamma}$, which has the same effect as minimzing $\|w'\|^2$. \\
So the original problem is equivalent to
\begin{equation}
\begin{aligned}
& \min_{w',\alpha'}
& & \|w'\|^2 \\
& \text{subject to}
& & y_i(x_i\cdot w'+\alpha')\geq 1,\ \forall i\in\{1,2,\cdots,n\} \\
\end{aligned}
\end{equation}

\newpage