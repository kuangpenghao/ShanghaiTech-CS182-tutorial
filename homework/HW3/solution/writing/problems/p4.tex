\item \defpoints{10} [Probability and Estimation]

The Poisson distribution is a useful discrete distribution which can be used to model the number of occurrences of something per unit time. For example, in networking, the number of packets to arrive in a given time window is often assumed to follow a Poisson distribution.
$\mathcal{D}=\{ x_{1}, x_{2}, \ldots, x_{n} \}, n>1$ are i.i.d. samples from exponential distribution with parameter $\lambda > 0$, i.e., $X \sim \text{Expo}(\lambda)$. Recall the PDF of exponential distribution is
$$p(x\mid \lambda) = \begin{cases}
\lambda e^{-\lambda x},&\quad x > 0 \\
0,&\quad \text{otherwise}
\end{cases}$$

\begin{itemize}
\item[(a)] To derive the posterior distribution of $\lambda$, we assume its prior distribution follows gamma distribution with parameters $\alpha,\beta > 0$, i.e., $\lambda \sim\text{Gamma}(\alpha,\beta)$ (since the range of gamma distribution is also $(0,+\infty)$, thus it's a plausible assumption). The PDF of $\lambda$ is given by
\begin{align*}
p(\lambda\mid \alpha,\beta) &= \frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha-1}e^{-\lambda\beta} \\
\text{where \ \ \ \ } \Gamma(\alpha) &= \int_{0}^{+\infty} t^{\alpha-1}e^{-t}dt,\ \alpha>0.
\end{align*}
Show that the posterior distribution $p(\lambda\mid \mathcal{D})$ is also a gamma distribution and identify its parameters. Hints: Feel free to drop constants. \defpoints{5}

\item[(b)] Derive the maximum a posterior (MAP) estimation for $\lambda$ under $\text{Gamma}(\alpha,\beta)$ prior. \defpoints{5}

\end{itemize}

\solution

(a) From Bayes' Rule, we can get that
$$p(\lambda|\mathcal{D})=\dfrac{p(\mathcal{D}|\lambda)p(\lambda)}{p(\mathcal{D})}$$
Since $\mathcal{D}$ do not contain any $\lambda$, so we can get that
$$p(\lambda|\mathcal{D})\propto p(\mathcal{D}|\lambda)p(\lambda)$$
And since $\mathcal{D}=\{x_1,x_2,\cdots,x_n\}$ are i.i.d. samples from exponential distribution with parameter $\lambda > 0$, so we can get that
$$p(\mathcal{D}|\lambda)=p(x_1,x_2,\cdots,x_n|\lambda)=\prod\limits_{i=1}^np(x_i|\lambda)$$
Since $p(x|\lambda)=\lambda e^{-\lambda x},x>0$, so WLOG, we can assume that all the sampling points are positive,
i.e. $\forall i, x_i>0$, then we can get that
$$p(\mathcal{D}|\lambda)=\prod\limits_{i=1}^n\lambda e^{-\lambda x_i}=\lambda^n e^{-\lambda\sum\limits_{i=1}^nx_i}$$
And since we know that the prior distribution of $\lambda$ is that $\lambda\sim \text{Gamma}(\alpha,\beta)$,
so we can get that
$$p(\lambda)=p(\lambda|\alpha,\beta)=\dfrac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha-1}e^{-\lambda\beta}$$
So
$$p(\lambda|\mathcal{D})\propto \lambda^n e^{-\lambda\sum\limits_{i=1}^nx_i}\cdot\dfrac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha-1}e^{-\lambda\beta}\propto\lambda^{n+\alpha-1}e^{-\lambda\left(\sum\limits_{i=1}^nx_i+\beta\right)}$$
Since $p(\lambda|\mathcal{D})$ is in terms of conditional probability, so its distribution must be a valid distribution.\\
And from $$p(\lambda|\mathcal{D})\propto\lambda^{n+\alpha-1}e^{-\lambda\left(\sum\limits_{i=1}^nx_i+\beta\right)}$$
we can get that the distribution is $$p(\lambda|\mathcal{D})\sim \text{Gamma}\left(\alpha+n,\beta+\sum\limits_{i=1}^nx_i\right)$$

So above all, we have proved that the posterior distribution $p(\lambda|\mathcal{D})$ is also a Gamma distribution, and the parameters is that $p(\lambda|\mathcal{D})\sim \text{Gamma}\left(\alpha+n,\beta+\sum\limits_{i=1}^nx_i\right)$

(b) From (a), we get that $p(\lambda|\mathcal{D})\sim \text{Gamma}\left(\alpha+n,\beta+\sum\limits_{i=1}^nx_i\right)$.\\
and $p(\lambda|\mathcal{D})\propto\lambda^{n+\alpha-1}e^{-\lambda\left(\sum\limits_{i=1}^nx_i+\beta\right)}$.\\
So the MAP for $\lambda$ under $\text{Gamma}(\alpha,\beta)$ prior is that
$$\hat{\lambda}_{MAP}=\argmax\limits_{\lambda}\lambda^{\alpha+n-1}e^{-\lambda\left(\beta+\sum\limits_{i=1}^nx_i\right)}$$
Take it into the log-likelyhood function, the result of MAP is the same.\\
So
$$\hat{\lambda}_{MAP}=\argmax\limits_{\lambda}(\alpha+n-1)\log\lambda-\left(\beta+\sum\limits_{i=1}^nx_i\right)\lambda$$
Let $$f(\lambda)=(\alpha+n-1)\log\lambda-\left(\beta+\sum\limits_{i=1}^nx_i\right)\lambda$$
then $$f'(\lambda)=\dfrac{\alpha+n-1}{\lambda}-\left(\beta+\sum\limits_{i=1}^nx_i\right)$$
And $$f''(\lambda)=-(\alpha+n-1)\dfrac{1}{\lambda^2}<0$$
So we could find that the function $f(\lambda)$ is a concave function.\\
So to get the MAP, we need to find the point where the first derivative of $f(\lambda)$ is equal to 0.\\
i.e. $$\dfrac{\alpha+n-1}{\lambda}-\left(\beta+\sum\limits_{i=1}^nx_i\right)=0$$
So $$\hat{\lambda}_{MAP}=\dfrac{\alpha+n-1}{\beta+\sum\limits_{i=1}^nx_i}$$

So above all, the MAP estimation for $\lambda$ under $\text{Gamma}(\alpha,\beta)$ prior is that
$$\hat{\lambda}_{MAP}=\dfrac{\alpha+n-1}{\beta+\sum\limits_{i=1}^nx_i}$$

\newpage