\item \defpoints{15} [Perceptron Learning Algorithm]
Consider a binary classification problem. The input space is $\mathbb{R}^{d}$. The output space is $\{ +1, -1 \}$. For simplicity, we modified the input to be $\mathbf{x} = [x_0, x_1, \cdots, x_d]^{\top}$ with $x_0=1$. The output is predicted using the hypothesis:
\begin{equation}
    h(\mathbf{x}) = \text{sign}(\mathbf{w}^{\top}\mathbf{x}),
\end{equation}
where $\mathbf{w} = [w_0, w_1, \cdots, w_d]^{\top}$ and $w_0$ is the bias.

The \textit{perceptron learning algorithm} determines $\mathbf{w}$ using a simple iterative method. Here is how it works. At iteration $t$, where $t=0,1,2, \ldots$, there is a current value of the weight vector, call it $\mathbf{w}(t)$. The algorithm picks an example from $\left(\mathbf{x}_1, y_1\right) \cdots\left(\mathbf{x}_N, y_N\right)$ that is currently misclassified, call it $(\mathbf{x}(t), y(t))$, and uses it to update $\mathbf{w}(t)$. Since the example is misclassified, we have $y(t) \neq$ $\operatorname{sign}\left(\mathbf{w}^{\top}(t) \mathbf{x}(t)\right)$. The update rule is

\begin{equation}
    \mathbf{w}(t+1)=\mathbf{w}(t)+y(t) \mathbf{x}(t).
\end{equation}

\begin{itemize}
\item[(a)] Show that $y(t) \mathbf{w}^{\top}(t) \mathbf{x}(t)<0$. [Hint: $\mathbf{x}(t)$ is misclassified by $\mathbf{w}(t)$.] ~\defpoints{5}
\item[(b)] Show that $y(t) \mathbf{w}^{\top}(t+1) \mathbf{x}(t)>y(t) \mathbf{w}^{\top}(t) \mathbf{x}(t)$. ~\defpoints{5}
\item[(c)]   As far as classifying $\mathbf{x}(t)$ is concerned, argue that the move from $\mathbf{w}(t)$ to $\mathbf{w}(t+1)$ is a move ``in the right direction". ~\defpoints{5}
\end{itemize}

\solution

(a) Since we are considering the misclassified, so we have $y(t) \neq \operatorname{sign}\left(\mathbf{w}^{\top}(t) \mathbf{x}(t)\right)$.\\
And since $y(t), \operatorname{sign}\left(\mathbf{w}^{\top}(t) \mathbf{x}(t)\right)\in \{+1, -1\}$, so we have $y(t)\cdot\operatorname{sign}\left(\mathbf{w}^{\top}(t)\mathbf{x}(t)\right)=-1<0$.\\
Suppose that $\operatorname{sign}\left(\mathbf{w}^{\top}(t)\mathbf{x}(t)\right)=k\cdot \mathbf{w}^{\top}(t) \mathbf{x}(t)$, where $k>0$.\\
So $y(t)\cdot\operatorname{sign}\left(\mathbf{w}^{\top}(t)\mathbf{x}(t)\right)=y(t)\cdot k\cdot \mathbf{w}^{\top}(t) \mathbf{x}(t)=k\cdot y(t) \mathbf{w}^{\top}(t) \mathbf{x}(t)<0$.\\
Since $k>0$, so we have
$$y(t) \mathbf{w}^{\top}(t) \mathbf{x}(t)<0$$
So above all, we have proved that $y(t) \mathbf{w}^{\top}(t) \mathbf{x}(t)<0$.

(b) Since we are considering the misclassified, so we have $\mathbf{w}(t+1)=\mathbf{w}(t)+y(t) \mathbf{x}(t)$.\\
So
$$y(t)\mathbf{w}^{\top}(t+1)\mathbf{x}(t)=y(t)\mathbf{w}^{\top}(t)\mathbf{x}(t)+y(t)y(t)\mathbf{x}^{\top}(t)\mathbf{x}(t)
=y(t)\mathbf{w}^{\top}(t)\mathbf{x}(t)+y^2(t)\|\mathbf{x}(t)\|^2$$
Since $y(t)\in \{+1, -1\}$, so we have $y^2(t)=1$.\\
And since for the simplicity, we have the input to be $\mathbf{x} = [x_0, x_1, \cdots, x_d]^{\top}$ with $x_0=1$, so we have
$\mathbf{x}\neq\mathbf{0}$, i.e. $\|\mathbf{x}(t)\|^2>0$.\\
So we have
$$y^2(t)\|\mathbf{x}(t)\|^2>0$$
So
$$y(t)\mathbf{w}^{\top}(t+1)\mathbf{x}(t)=y(t)\mathbf{w}^{\top}(t)\mathbf{x}(t)+y^2(t)\|\mathbf{x}(t)\|^2>y(t)\mathbf{w}^{\top}(t)\mathbf{x}(t)$$
So above all, we have proved that $y(t) \mathbf{w}^{\top}(t+1) \mathbf{x}(t)>y(t) \mathbf{w}^{\top}(t) \mathbf{x}(t)$.

(c) We only consider about the misclassified case.\\
From (a), we knew that $$y(t) \mathbf{w}^{\top}(t) \mathbf{x}(t)<0$$
And from (b), we knew that
$$y(t) \mathbf{w}^{\top}(t+1) \mathbf{x}(t)>y(t) \mathbf{w}^{\top}(t) \mathbf{x}(t)$$
So we could see that the move from $\mathbf{w}(t)$ to $\mathbf{w}(t+1)$ is making the $y(t)\mathbf{w}^{\top}\mathbf{x}(t)$ to the more positive direction, and since if $y(t)\mathbf{w}^{\top}\mathbf{x}(t)>0$, then it is a correct classification. \\
And if the total input data are linearly separable, from what we have learned, we could get that with at most $M=(\frac{R}{\gamma})^2$ such misclassified's movement, where $R$ is the radius of the smallest sphere that contains all the input data, and $\gamma$ is the margin, then we could get the correct classification.\\

So above all, we could say that the move from $\mathbf{w}(t)$ to $\mathbf{w}(t+1)$ is a move ``in the right direction".

\newpage