{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6280b8ea",
      "metadata": {
        "id": "6280b8ea"
      },
      "source": [
        "# HW5 Coding Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0691f9",
      "metadata": {
        "id": "dc0691f9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1a3322",
      "metadata": {
        "id": "ba1a3322"
      },
      "source": [
        "## Problem 0: Pytorch Tutorial (12 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a5eb06",
      "metadata": {
        "id": "c3a5eb06"
      },
      "source": [
        "### Tensors\n",
        "Tensors can be created from numpy data or by using pytorch directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0be8dec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0be8dec",
        "outputId": "c715c1be-bf46-4410-80e7-0a0829917de4"
      },
      "outputs": [],
      "source": [
        "x_data = [[1, 2], [3, 4]]\n",
        "x = torch.tensor(x_data)\n",
        "\n",
        "np_array = np.array(x_data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "\n",
        "shape = (2,3)\n",
        "rand_tensor = torch.rand(shape)\n",
        "np_rand_array = rand_tensor.numpy()\n",
        "\n",
        "print(f\"Tensor from np: \\n {x_np} \\n\")\n",
        "print(f\"Rand Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Rand Numpy Array: \\n {np_rand_array} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447a6172",
      "metadata": {
        "id": "447a6172"
      },
      "source": [
        "#### 1) Tensor squeezing, unsqueezing and viewing (3 points)\n",
        "\n",
        "Tensor squeezing, unsqueezing and viewing are important methods to change the dimension of a Tensor, and the corresponding functions are [torch.squeeze](https://pytorch.org/docs/stable/torch.html#torch.squeeze), [torch.unsqueeze](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze) and [torch.Tensor.view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view). Please read the documents of the functions, and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c17808f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c17808f",
        "outputId": "66945ec3-023a-45bb-f8c4-14d252c919da"
      },
      "outputs": [],
      "source": [
        "# x is a tensor with size being (3, 2)\n",
        "x = torch.Tensor([[1, 2],[3, 4],[5, 6]])\n",
        "print(\"Original shape:\", x.shape)  # Should be (3, 2)\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Add two new dimensions to x by using the function torch.unsqueeze(input, dim) -> Tensor, so that the size of x becomes (3, 1, 2, 1).\n",
        "# Hint: Add dimension at position 1 (second dimension)\n",
        "# Hint: Add dimension at position 3 (fourth dimension)\n",
        "\n",
        "############## END YOUR CODE ##############\n",
        "print(x.shape) # Should be torch.Size([3, 1, 2, 1])\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Remove the two dimensions just added by using the function torch.squeeze(input, dim) -> Tensor, and change the size of x back to (3, 2).\n",
        "# Hint: Remove the dimension at position 3\n",
        "# Hint: Remove the dimension at position 1\n",
        "\n",
        "############## END YOUR CODE ##############\n",
        "print(x.shape) # Should be torch.Size([3, 2])\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: x is now a two-dimensional tensor, or in other words a matrix. Now use the function torch.Tensor.view(*shape) and change x to a one-dimensional vector with size being (6).\n",
        "\n",
        "############## END YOUR CODE ##############\n",
        "print(\"After view:\", x.shape)  # Should be torch.Size([6])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a0d11f5",
      "metadata": {
        "id": "5a0d11f5"
      },
      "source": [
        "### 2) Tensor concatenation and stack (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c62d5b8",
      "metadata": {
        "id": "4c62d5b8"
      },
      "source": [
        "Tensor concatenation and stack are operations to combine small tensors into big tensors. The corresponding functions are [torch.cat](https://pytorch.org/docs/stable/torch.html#torch.cat) and [torch.stack](https://pytorch.org/docs/stable/torch.html#torch.stack). Please read the documents of the functions, and finish the following practice.\n",
        "\n",
        "**Hints:**<br>\n",
        "1. `torch.stack((obj1, obj2))`: A new dimension is automatically added (dim=0 by default) and the input tensors are then stacked along that dimension.\n",
        "2. `torch.cat((obj1, obj2))`: Dimensions are not added, but are spliced directly over existing dimensions, you should consider this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbfbb58f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbfbb58f",
        "outputId": "c59c496c-179b-494e-ac77-4a5bc39a30f2"
      },
      "outputs": [],
      "source": [
        "# x is a tensor with size being (3, 2)\n",
        "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# y is a tensor with size being (3, 2)\n",
        "y = torch.Tensor([[-1, -2], [-3, -4], [-5, -6]])\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Our goal is to generate a tensor z with size as (2, 3, 2), and z[0,:,:] = x, z[1,:,:] = y. Use torch.stack to generate such a z\n",
        "z = ... # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print(z)\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Use torch.cat and torch.unsqueeze to generate z\n",
        "z = ... # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print(z)\n",
        "\n",
        "# The tow outputs are expected to be tensor([[[ 1.,  2.], [ 3.,  4.], [ 5.,  6.]], [[-1., -2.], [-3., -4.], [-5., -6.]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e253f2c6",
      "metadata": {
        "id": "e253f2c6"
      },
      "source": [
        "#### 3) Tensor expansion (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0057816c",
      "metadata": {
        "id": "0057816c"
      },
      "source": [
        "Tensor expansion is to expand a tensor into a larger tensor along singleton dimensions. The corresponding functions are [torch.Tensor.expand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand) and [torch.Tensor.expand_as](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as). Please read the documents of the functions, and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f833a39d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f833a39d",
        "outputId": "2666562c-325e-4d13-824c-048668752c08"
      },
      "outputs": [],
      "source": [
        "# x is a tensor with size being (3)\n",
        "x = torch.Tensor([1, 2, 3])\n",
        "\n",
        "# Our goal is to generate a tensor z with size (2, 3), so that z[0,:,:] = x, z[1,:,:] = x.\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Change the size of x into (1, 3) by using torch.unsqueeze.\n",
        "x = ...  # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print(x) # Output is expected to be tensor([[1., 2., 3.]])\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Then expand the new tensor to the target tensor by using torch.Tensor.expand.\n",
        "z = ...  # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print(z) # Output is expected to be tensor([[1., 2., 3.], [1., 2., 3.]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be27d3f",
      "metadata": {
        "id": "2be27d3f"
      },
      "source": [
        "#### 4) Tensor reduction in a given dimension (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc6b2b4",
      "metadata": {
        "id": "dcc6b2b4"
      },
      "source": [
        "In deep learning, we often need to compute the mean/sum/max/min value in a given dimension of a tensor. Please read the document of [torch.mean](https://pytorch.org/docs/stable/torch.html#torch.mean), [torch.sum](https://pytorch.org/docs/stable/torch.html#torch.sum), [torch.max](https://pytorch.org/docs/stable/torch.html#torch.max), [torch.min](https://pytorch.org/docs/stable/torch.html#torch.min), [torch.topk](https://pytorch.org/docs/stable/torch.html#torch.topk), and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a48592c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a48592c",
        "outputId": "a27f4911-9fbd-4282-bdbb-e85b584ba782"
      },
      "outputs": [],
      "source": [
        "# x is a random tensor with size being (10, 50)\n",
        "x = torch.randn(10, 50)\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Compute the mean value for each row of x.\n",
        "# You need to generate a tensor x_mean of size (10), and x_mean[k, :] is the mean value of the k-th row of x.\n",
        "# dim = 1: eliminate the second(1)'s dimension\n",
        "x_mean = ...  # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print(x_mean.shape)\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Compute the sum value for each row of x.\n",
        "# You need to generate a tensor x_sum of size (10).\n",
        "x_sum = ...  # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print(x_sum.shape)\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Compute the max value for each row of x.\n",
        "# You need to generate a tensor x_max of size (10).\n",
        "# Hint: torch.max() -> (max_val, indices)\n",
        "(x_max, indices) = ...  # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print(x_max.shape)\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Compute the min value for each row of x.\n",
        "# You need to generate a tensor x_min of size (10).\n",
        "# Hint: torch.max() -> (min_val, indices)\n",
        "(x_min, indices) = ...  # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print(x_min.shape)\n",
        "\n",
        "############## YOUR CODE HERE ##############\n",
        "# TODO: Compute the top-5 values for each row of x.\n",
        "# You need to generate a tensor x_min of size (10. 5).\n",
        "# Hint: torch.max() -> (min_val, indices)\n",
        "(x_xtop, indices) = ...  # Fill in this\n",
        "############## END YOUR CODE ##############\n",
        "print((x_xtop.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef466b57",
      "metadata": {
        "id": "ef466b57"
      },
      "source": [
        "### Autograd (0 point) (Highly recommend checking it out)\n",
        "This small section shows you how pytorch computes gradients. When we create tenors, we can set `requires_grad` to be true to indicate that we are using gradients. For most of the work that you actually do, you will use the `nn` package, which automatically sets all parameter tensors to have `requires_grad=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ac63b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7ac63b5",
        "outputId": "f3574cb2-56f1-4c90-8855-2831f56e02b0"
      },
      "outputs": [],
      "source": [
        "# Below is an example of computing the gradient for a single data point in logistic regression using pytorch's autograd.\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(1)  # label\n",
        "# requires_grad = True : we are using this parameter's gradient\n",
        "# use nn package, set all parameter tensors to have required_grad = True\n",
        "w = torch.randn(5, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "pred = torch.sigmoid(torch.matmul(x, w) + b)\n",
        "loss = torch.nn.functional.binary_cross_entropy(pred, y)\n",
        "loss.backward()  # Computers gradients\n",
        "print(\"W gradient:\", w.grad)\n",
        "print(\"b gradient:\", b.grad)\n",
        "\n",
        "# when we want to actually take an update step, we can use optimizers:\n",
        "optimizer = torch.optim.SGD([w, b], lr=0.1)  # [w,b] is the model.parameters()\n",
        "print(\"Weight before\", w)\n",
        "optimizer.step()  # use the computed gradients to update\n",
        "# Print updated weights\n",
        "print(\"Updated weight\", w)\n",
        "\n",
        "# Performing operations with gradients enabled is slow...\n",
        "# You can disable gradient computation using the following enclosure:\n",
        "with torch.no_grad():\n",
        "    # Perform operations without gradients\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72f716c9",
      "metadata": {
        "id": "72f716c9"
      },
      "source": [
        "### Devices (0 point) (Highly recommend checking it out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcefaa78",
      "metadata": {
        "id": "bcefaa78"
      },
      "source": [
        "Pytorch supports accelerating computation using GPUs which are available on google colab. To use a GPU on google colab, go to runtime -> change runtime type -> select GPU.\n",
        "\n",
        "Note that there is some level of strategy for knowing when to use which runtime type. Colab will kick users off of GPU for a certain period of time if you use it too much. Thus, its best to run simple models and prototype to get everything working on CPU, then switch the instance type over to GPU for training runs and parameter tuning.\n",
        "\n",
        "Its best practice to make sure your code works on any device (GPU or CPU) for pytorch, but note that numpy operations can only run on the CPU. Here is a standard flow for using GPU acceleration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0f573f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b0f573f",
        "outputId": "f6315d3d-6350-4a32-c22f-5966dba4a08d"
      },
      "outputs": [],
      "source": [
        "# Determine the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", device)\n",
        "# Next create your tensors\n",
        "tensor = torch.zeros(4, 4, requires_grad=True)\n",
        "# Move the tensor to the device you want to use\n",
        "tensor = tensor.to(device)\n",
        "\n",
        "# Perform whatever operations you want.... (often this will involve gradients)\n",
        "# These operations will be accelerated by GPU.\n",
        "tensor = 10*(tensor + 1)\n",
        "\n",
        "# bring the tensor back to CPU, first detaching it from any gradient computations\n",
        "tensor = tensor.detach().cpu()\n",
        "\n",
        "# Convert to numpy if you want to perform numpy operations.\n",
        "tensor_np = tensor.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7dcfa9",
      "metadata": {
        "id": "1c7dcfa9"
      },
      "source": [
        "### Build an NN (0 point) (Highly recommend checking it out)\n",
        "Pytorch implements composable blocks in `Module` classes. All layers and modules in pytorch inherit from `nn.Module`. When you make a module you need to implement two functions: `__init__(self, *args, **kwargs)` and `foward(self, *args, **kwargs)`. Modules also have some nice helper functions, namely `parameters` which will recursively return all of the parameters. Here is an example of a logistic regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f267628",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f267628",
        "outputId": "18e3d51c-17b1-45c6-b18f-31b64833b940"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "  def __init__(self, in_dim):\n",
        "    super().__init__()\n",
        "    # This is a linear layer, it computes Xw + b\n",
        "    self.layer = nn.Linear(in_dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.sigmoid(self.layer(x)).squeeze(-1)\n",
        "\n",
        "\n",
        "perceptron = Perceptron(10)\n",
        "# Move all the perceptron's tensors to the device\n",
        "perceptron = perceptron.to(device)\n",
        "# module.parameters() return all the parameters in this module ; There W and b\n",
        "print(\"Parameters\", list(perceptron.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "955aa3be",
      "metadata": {
        "id": "955aa3be"
      },
      "source": [
        "### Datasets (0 point) (Highly recommend checking it out)\n",
        "Pytorch has nice interfaces for using datasets. Suppose we create a logistic regression dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2c85b34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "b2c85b34",
        "outputId": "2b18b372-4f07-4ed2-d504-d07cbb2de062"
      },
      "outputs": [],
      "source": [
        "c1_x1, c1_x2 = np.random.multivariate_normal(\n",
        "    [-2.5, 3], [[1, 0.3], [0.3, 1]], 500).T\n",
        "c2_x1, c2_x2 = np.random.multivariate_normal([1, 1], [[2, 1], [1, 2]], 500).T\n",
        "c1_X = np.vstack((c1_x1, c1_x2)).T\n",
        "c2_X = np.vstack((c2_x1, c2_x2)).T\n",
        "X = np.concatenate((c1_X, c2_X))\n",
        "y = np.concatenate((np.zeros(500), np.ones(500)))\n",
        "# Shuffle the data\n",
        "permutation = np.random.permutation(X.shape[0])\n",
        "X = X[permutation, :]\n",
        "y = y[permutation]\n",
        "# Plot the data\n",
        "plt.plot(c1_x1, c1_x2, 'x')\n",
        "plt.plot(c2_x1, c2_x2, 'o')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b83f341",
      "metadata": {
        "id": "7b83f341"
      },
      "source": [
        "We can then create a pytorch dataset object as follows. Often times, the default pytorch datasets will create these objects for you. Then, we can apply dataloaders to iterate over the dataset in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7acbef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7acbef4",
        "outputId": "b2ad75ba-33bd-4444-fd69-e0ffddc50d49"
      },
      "outputs": [],
      "source": [
        "dataset = torch.utils.data.TensorDataset(\n",
        "    torch.from_numpy(X), torch.from_numpy(y))\n",
        "print(dataset)\n",
        "# We can create a dataloader that iterates over the dataset in batches.\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "for x, y in dataloader:\n",
        "    print(\"Batch x:\", x)\n",
        "    print(\"Batch y:\", y)\n",
        "    break\n",
        "\n",
        "# Clean up the dataloader as we make a new one later, you can ignore it here\n",
        "del dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d9eaae",
      "metadata": {
        "id": "e7d9eaae"
      },
      "source": [
        "Splitting Train, Validation and Test sets randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e4123e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e4123e",
        "outputId": "d10d5921-deda-4151-a99d-604b82d24981"
      },
      "outputs": [],
      "source": [
        "#Training: 70%, Validation: 15%, Testing: 15%\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size]\n",
        ")\n",
        "\n",
        "# Creat the data_loders\n",
        "batch_size = 10\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False)\n",
        "\n",
        "print(f\"Training size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")\n",
        "print(f\"Testing size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "068588b7",
      "metadata": {
        "id": "068588b7"
      },
      "source": [
        "### <font color='red'>Training Loop and Progress Bar (0 point) (Very important! Highly recommend checking it out)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d9bc50",
      "metadata": {
        "id": "a2d9bc50"
      },
      "source": [
        "Here is an example of training a full Logistic Regression model in pytorch. Note the extensive use of modules -- modules can be used for storing networks, computation steps etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ffa39b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ffa39b0",
        "outputId": "9a75bb71-02f2-48dd-83e0-2ec59e8072c1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", device)\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "num_features = dataset[0][0].shape[0]\n",
        "model = Perceptron(num_features).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "model.train()  # Put model in training mode\n",
        "for epoch in range(epochs):\n",
        "    training_losses = []\n",
        "    train_correct = 0\n",
        "    ProgressBar = tqdm(train_loader)\n",
        "    for x, y in ProgressBar:\n",
        "        x, y = x.float().to(device), y.float().to(device)\n",
        "        #  for every mini-batch during the training phase, we typically want to explicitly set the gradients to zero before starting to do backpropagation\n",
        "        # Remove the gradients from the previous step ;Sets the gradients of all optimized torch.Tensor s to zero.\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)   # The value predicted using our model\n",
        "        # compute loss\n",
        "        loss = criterion(pred, y)\n",
        "        # Compute gradients.\n",
        "        loss.backward()\n",
        "        # update the parameter using the gradient computed\n",
        "        optimizer.step()\n",
        "        training_losses.append(loss.item())\n",
        "        # In a classification task, the output of a neural network model is typically the scores or probabilities for each class.\n",
        "        # For example, in a 10-class classification task, the output of the last layer of the model is a tensor with the shape of (batch_size, 10). For each sample (i.e., each row in pred), we can obtain the predicted class index of each sample by using torch.argmax(pred, dim=1).\n",
        "        # So for multi-class classification tasks, try to use: train_correct += torch.sum(torch.argmax(pred, dim=1) == y).item()\n",
        "        train_correct += torch.sum(torch.round(pred) == y).item()\n",
        "    train_accuracy = train_correct / len(train_dataset)\n",
        "    val_losses = []\n",
        "    num_correct = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()  # Put model in eval mode\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val, y_val = x_val.float().to(device), y_val.float().to(device)\n",
        "            pred_val = model(x_val)\n",
        "            loss_val = criterion(pred_val, y_val)\n",
        "            val_losses.append(loss_val.item())\n",
        "            num_correct += torch.sum(torch.round(pred_val) == y_val).item()\n",
        "        model.train()  # Put model back in train mode\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "          f\"Train Loss: {np.mean(training_losses):.4f} | \"\n",
        "          f\"Train Acc: {train_accuracy:.4f} | \"\n",
        "          f\"Val Loss: {np.mean(val_losses):.4f} | \"\n",
        "          f\"Val Acc: {num_correct / len(val_dataset):.4f}\")\n",
        "\n",
        "# We can run predictions on the data to determine the Testing accuracy.\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.float().to(device), y.float().to(device)\n",
        "        pred = model(x)\n",
        "        test_correct += torch.sum(torch.round(pred) == y).item()\n",
        "\n",
        "print(f\"\\nTest Accuracy: {test_correct / len(test_dataset):.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3316220",
      "metadata": {
        "id": "f3316220"
      },
      "source": [
        "## Problem 1: MLP for FashionMNIST (18 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a305c856",
      "metadata": {
        "id": "a305c856"
      },
      "source": [
        "Now you will train a multi-layer perceptron model on the FashionMNIST dataset. Your deliverables are as follows:\n",
        "\n",
        "1. Code for training an MLP on FashionMNIST.\n",
        "2. A plot of the training and validation loss for at least 8 epochs.\n",
        "3. A plot of the training and validation accuracy for each epoch, achieving a final validation accuracy of at least 82% by the end of the training.\n",
        "\n",
        "Below we will create the training and validation datasets for you. It is on you to implement an MLP / Feed Forward neural network yourself. Please leverage the example training loop from above.\n",
        "\n",
        "Here are some pytorch components that you should definitely use:\n",
        "1. `nn.Linear`\n",
        "2. Some activation: `nn.ReLU`, `nn.Tanh`, `nn.Sigmoid`, etc.\n",
        "3. `nn.CrossEntropyLoss`\n",
        "\n",
        "Here are some challenges that you will need to overcome:\n",
        "1. The data is, by default, configured in image form, i.e. a (28 x 28) tensor per sample, instead of single feature vector. You will need to **reshape** it somewhere to feed it in as vector to the MLP. There are many ways of doing this according to **Problem 0**.\n",
        "2. You need to write code for plotting.\n",
        "3. You need to find the appropriate hyper-parameters to achieve good accuracy.\n",
        "\n",
        "Your underlying model must be fully connected or \"dense\", and may not use any convolutions etc., but you can use anything in `torch.optim` or any layers in `torch.nn` besides `nn.Linear` that do not have weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd37cc95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd37cc95",
        "outputId": "0a7465d5-83f5-4f20-9a55-964b3dd7befd"
      },
      "outputs": [],
      "source": [
        "# Creating the datasets\n",
        "# feel free to modify this as you see fit.\n",
        "transform = torchvision.transforms.ToTensor()\n",
        "\n",
        "training_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=False,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "validation_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=False,\n",
        "    transform=transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92de4324",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "92de4324",
        "outputId": "8ed31c2d-3123-4b3a-d013-38ac857566c7"
      },
      "outputs": [],
      "source": [
        "images = [training_data[i][0] for i in range(9)]\n",
        "plt.imshow(torchvision.utils.make_grid(torch.stack(images),\n",
        "           nrow=3, padding=5).numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70cc212d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70cc212d",
        "outputId": "af7c3ac7-5a7b-4c5e-d12e-b06ae92997b1"
      },
      "outputs": [],
      "source": [
        "# Get the knowledge of the Training and Validation Set\n",
        "print(\"number of training samples: \" + str(len(training_data)) + \"\\n\" +\n",
        "      \"number of validation samples: \" + str(len(validation_data)))\n",
        "print(\"datatype of the 1st training sample: \", training_data[0][0].type())\n",
        "print(\"size of the 1st training sample: \", training_data[0][0].size())\n",
        "\n",
        "# Find out how many categories in the sample.\n",
        "max_label = float(\"-inf\")\n",
        "min_label = float(\"inf\")\n",
        "for i in range(len(training_data)):\n",
        "  if training_data[i][1] > max_label:\n",
        "    max_label = training_data[i][1]\n",
        "  if training_data[i][1] < min_label:\n",
        "    min_label = training_data[i][1]\n",
        "print(\"max_label = \"+str(max_label))\n",
        "print(\"min_label = \"+str(min_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70472772",
      "metadata": {
        "id": "70472772"
      },
      "source": [
        "In the cell below, you'll implement a MLP. Please follow the guidence in comments. <br>\n",
        "**Scoring criteria:**<br>\n",
        "**1. Print out the train loss, train accuracy, validation loss, and validation accuracy for each epoch. (8 points)**<br>\n",
        "**2. Grading criteria: <br>\n",
        "At the last epoch, if the validation accuracy (validation_acc) of your model is greater than or equal to 0.86, you will get `10 points`. <br>\n",
        "If 0.83 <= validation_acc < 0.86, you will get `6 points`. <br>\n",
        "If 0.80 <= validation_acc < 0.83, you will get `4 points`. <br>\n",
        "If validation_acc < 0.80, you will not get any score. <br>\n",
        "You are free to use the *early stopping* strategy that can prevent overfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9ab9a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb9ab9a4",
        "outputId": "c24418f2-3224-4eb3-c8d7-d0a6ea3c70fd"
      },
      "outputs": [],
      "source": [
        "############## YOUR CODE HERE ##############\n",
        "# In this part, please follow our instruction step by step to get familiar with Pytorch, which would reduce your workload.\n",
        "\n",
        "# 1. Confirm whether the CUDA is available or not.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", device)\n",
        "\n",
        "# 2. TODO: Encapsulate your Multilayer Perceptron (MLP) model within a class.\n",
        "# This class should include a constructor function __init__(...) and a forward(...) function that is used to carry out forward propagation.\n",
        "# You can modify these two functions' input parameters.\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "    ######## YOUR CODE HERE ########\n",
        "        pass\n",
        "    ######## END YOUR CODE ########\n",
        "\n",
        "    def forward(self, x):\n",
        "    ######## YOUR CODE HERE ########\n",
        "        pass\n",
        "    ######## END YOUR CODE ########\n",
        "\n",
        "\n",
        "# 3. TODO: Initialize your model with proper input size and output size\n",
        "######## YOUR CODE HERE ########\n",
        "n_inputs = ...\n",
        "n_outputs = ...\n",
        "model = MLP(n_inputs, n_outputs).to(device)\n",
        "####### END YOUR CODE ########\n",
        "\n",
        "# 4. TODO: Define the Training Parameters like epochs, batch_size, learning_rate, optimizer, criterion_loss, etc.\n",
        "######## YOUR CODE HERE ########\n",
        "epochs = ...\n",
        "batch_size = ...\n",
        "learning_rate = ...\n",
        "optimizer = ...\n",
        "criterion = ...\n",
        "######## END YOUR CODE ########\n",
        "\n",
        "# 5. TODO: Put the training and testing data into a DataLoader\n",
        "# you can use torch.utils.data.DataLoader() to complete this step.\n",
        "######## YOUR CODE HERE ########\n",
        "train_loader = ...\n",
        "validation_loader = ...\n",
        "######## END YOUR CODE ########\n",
        "\n",
        "# 6. Training\n",
        "# During the training process, make sure to add the training accuracy, validation accuracy, training loss, and validation loss to the list below.\n",
        "# This will allow you to visualize these metrics and assess whether the model is overfitting.\n",
        "train_acc = []\n",
        "valid_acc = []\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "\n",
        "model.train()  # Put model in training mode\n",
        "for epoch in range(epochs):\n",
        "    # TODO: Please mimic the training example of logistic regression above to write the training code here.\n",
        "    ######## YOUR CODE HERE ########\n",
        "    pass\n",
        "    ######## END YOUR CODE ########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b502e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "42b502e7",
        "outputId": "f5d836b1-e78f-4cb3-8482-9dc706242df4"
      },
      "outputs": [],
      "source": [
        "# plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(train_loss, label='Training_Loss')\n",
        "plt.plot(valid_loss, label='Validation_Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(\"Loss with epoches\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(train_acc, label='Training_Acc')\n",
        "plt.plot(valid_acc, label='Validation_Acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(\"Accuracy with epoches\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbdb7e40",
      "metadata": {
        "id": "bbdb7e40"
      },
      "source": [
        "## Problem 2: CNN for CIFAR-10 (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef9b1ee",
      "metadata": {
        "id": "6ef9b1ee"
      },
      "source": [
        "In this section, you will construct a Convolutional Neural Network (CNN) for the CIFAR - 10 dataset. You have already utilized this dataset in the coding part of Homework 2. However, in this particular part, there is no need to download the dataset separately. It is advisable to employ GPU acceleration for this section to enhance the operational efficiency. Nevertheless, this is not a mandatory requirement.\n",
        "\n",
        "Here are some of the components you should consider using:\n",
        "1. `nn.Conv2d`\n",
        "2. `nn.ReLU`\n",
        "3. `nn.Linear`\n",
        "4. `nn.CrossEntropyLoss`\n",
        "5. `nn.MaxPooling2d` (Optional, many implementations without it exist)\n",
        "\n",
        "We encourage you to explore different ways of improving your model to obtain higher accuracies. Here are some suggestions for things to look into:\n",
        "1. Popular CNN architectures like ResNets, etc.\n",
        "2. Different optimizers and their parameters (see `torch.optim`)\n",
        "3. Image preprocessing / data augmentation (see `torchvision.transforms`)\n",
        "4. Regularization or dropout (see `torch.optim` and `torch.nn` respectively)\n",
        "5. Learning rate scheduling: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n",
        "Though we encourage you to explore, there are some rules:\n",
        "1. You are not allowed to use any pre-defined architectures or feature extractors in your network.\n",
        "2. You are not allowed to use **any** pretrained weights, i.e. no transfer learning.\n",
        "3. You cannot train on the test data (that would pretty much defeat the whole point of machine learning).\n",
        "\n",
        "<font color='red'>Scoring Criteria:</font>\n",
        "1. The final test accuracy of your model should be $\\geq 0.80$ to obtain **10 points**.<br>\n",
        "If $0.7\\leq$ test_acc $< 0.8$, you will get **5 points**. <br>\n",
        " If test_acc $< 0.7$, no score will be awarded.\n",
        "2. Print out the train loss, train accuracy, validation loss, and validation accuracy for each epoch.**(5 points)**\n",
        "3. Provide at least one training curve for your model. This curve should depict the training loss and validation loss per epoch or step after training for at least 10 epochs. **(5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b0dd8fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b0dd8fc",
        "outputId": "bdc96980-dd41-4de0-f0aa-c26b0f192322"
      },
      "outputs": [],
      "source": [
        "# Creating the datasets, feel free to change this as long as you do the same to the test data.\n",
        "# You can also modify this to split the data into training and validation.\n",
        "# See https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
        "\n",
        "# Training and Validation transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Testing transform\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Training and validation data\n",
        "train_val_data = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=False,\n",
        "    transform=transform_train,\n",
        ")\n",
        "\n",
        "train_size = int(0.9*len(train_val_data))\n",
        "val_size = len(train_val_data) - train_size\n",
        "\n",
        "# 9:1 randomly split the original training set to training and validation set\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "\n",
        "train_data, valid_data = torch.utils.data.random_split(\n",
        "    train_val_data,\n",
        "    [train_size, val_size],\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "# Official testing set.\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=False,\n",
        "    transform=transform_test,\n",
        ")\n",
        "\n",
        "print(f\"Training size: {len(train_data)}\")\n",
        "print(f\"Validation size: {len(valid_data)}\")\n",
        "print(f\"Testing size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f221191",
      "metadata": {
        "id": "5f221191"
      },
      "source": [
        "Again, let's first visualize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3793ac96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "3793ac96",
        "outputId": "05e5c254-1f89-4503-955d-994b6e4aee99"
      },
      "outputs": [],
      "source": [
        "images = [train_data[i][0] for i in range(9)]\n",
        "print(images[0].size())\n",
        "plt.imshow(torchvision.utils.make_grid(torch.stack(images),\n",
        "           nrow=3, padding=5).numpy().transpose((1, 2, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbc47b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cbc47b2",
        "outputId": "959fc0b4-e62d-4ed4-b911-ee6e09c7284f"
      },
      "outputs": [],
      "source": [
        "print(\"number of training samples: \" + str(len(train_data)) + \"\\n\" +\n",
        "      \"number of testing samples: \" + str(len(valid_data)))\n",
        "print(\"datatype of the 1st training sample: \", train_data[0][0].type())\n",
        "print(\"size of the 1st training sample: \", train_data[0][0].size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2561b26f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2561b26f",
        "outputId": "6de72b0a-a19f-4798-e29f-92d98a629c72"
      },
      "outputs": [],
      "source": [
        "max_label = float(\"-inf\")\n",
        "min_label = float(\"inf\")\n",
        "for i in range(len(train_data)):\n",
        "  if train_data[i][1] > max_label:\n",
        "    max_label = train_data[i][1]\n",
        "  if train_data[i][1] < min_label:\n",
        "    min_label = train_data[i][1]\n",
        "print(\"max_label = \"+str(max_label))\n",
        "print(\"min_label = \"+str(min_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15144a6b",
      "metadata": {
        "id": "15144a6b"
      },
      "source": [
        "### CNN Construction and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08cbfd4d",
      "metadata": {},
      "source": [
        "Here are some training tips for CNN training.\n",
        "1. **Optimization for Gradient Descent.**\n",
        "   1. **Stochastic gradient descent (SGD)** <br>\n",
        "    `torch.optim.SGD` <br>\n",
        "    When updating model parameters using only one training sample at each epoch, compared to traditional gradient descent methods that process the entire dataset, the memory and computational requirements are lower, which can accelerate convergence. Additionally, the randomness in the optimization process helps escape local minima. However, the noise in the updates may lead to a more erratic optimization path, unstable convergence speed, and the choice of learning rate becomes particularly crucial. <br>\n",
        "    Moreover, you can use SGD with momentum. The momentum parameter (typically between 0.9 and 0.99) controls the influence of past gradients on the current update, enabling the algorithm to persistently move in promising directions.\n",
        "    2. **RMSprop** <br>\n",
        "        `torch.optim.RMSprop`<br>\n",
        "        This method adaptively adjusts the learning rate based on the recent gradient history of parameters: it reduces the learning rate for parameters with consistently large gradients while increasing the update magnitude for those with smaller gradients, thereby balancing the step sizes of different parameter updates. The `alpha` parameter (decay rate, typically set to 0.9) in RMSprop controls how quickly old squared gradients are forgotten.\n",
        "    3. **Adam** <br>\n",
        "        `torch.optim.Adam`<br>\n",
        "        Adam optimizer efficiently updates parameters by combining momentum (1st-order moment) and RMSprop (2nd-order moment) with adaptive learning rates and bias correction.\n",
        "2. **Balancing hyperparameters (epochs, batch size, learning rate) is crucial during training.**\n",
        "3. **Dealing with Overfitting**: <br>\n",
        "    1. **Batch Normalization**: <br>\n",
        "        `torch.nn.BatchNorm2d`<br>\n",
        "        Batch normalization mitigates internal covariate shift by normalizing and rescaling layer inputs within each mini-batch.\n",
        "    2. **MaxPooling Layers**:  <br>\n",
        "        `nn.MaxPool2d`<br>\n",
        "        spatial downsampling\n",
        "    3. **Dropout**: <br>\n",
        "        `torch.nn.Dropout`<br>\n",
        "        Dropout randomly deactivates neurons during training to prevent overfitting and improve generalization.\n",
        "    4. **L1/L2 Normalization**:<br>\n",
        "        Configure the `weight_decay` parameter in the `optimizer`.\n",
        "    5. **Early Stopping**<br>\n",
        "        Implement early stopping by continuously monitoring validation performance and halting training when no improvement is observed for a predefined patience period.\n",
        "4. **Dealing with Underfitting**: <br>\n",
        "   1. **Deeper networks**\n",
        "   2. **More training epochs**\n",
        "   3. **Reasonably select activate functions**\n",
        "   4. **Use a classic network architecture, e.g. ResNet.**\n",
        "5. You are encouraged to add more optimization in order to reach your goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e13554",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1e13554",
        "outputId": "b4a0b23b-bd48-41d8-a1dd-d9e49b63e76b"
      },
      "outputs": [],
      "source": [
        "# 1. Confirm whether the CUDA is available or not.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", device)\n",
        "\n",
        "# 2. TODO: Encapsulate your Convolutional Neural Network (CNN) model within a class.\n",
        "# This class should include a constructor function __init__(...) and a forward(...) function that is used to carry out forward propagation.\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        ######## YOUR CODE HERE ########\n",
        "        pass\n",
        "        ######## END YOUR CODE ########\n",
        "\n",
        "    def forward(self, x):\n",
        "        ######## YOUR CODE HERE ########\n",
        "        pass\n",
        "        ######## END YOUR CODE ########\n",
        "\n",
        "\n",
        "# 3. Initialize your model\n",
        "model = CNN().to(device)\n",
        "\n",
        "# 4. TODO: Define the Training Parameters like epochs, batch_size, learning_rate, optimizer, criterion_loss, etc.\n",
        "######## YOUR CODE HERE ########\n",
        "epochs = ...\n",
        "batch_size = ...\n",
        "learning_rate = ...\n",
        "optimizer = ...\n",
        "criterion = ...\n",
        "######## END YOUR CODE ########\n",
        "\n",
        "# 5. TODO: Put the training and testing data into a DataLoader\n",
        "# you can use torch.utils.data.DataLoader() to complete this step.\n",
        "######## YOUR CODE HERE ########\n",
        "train_loader = ... \n",
        "validation_loader = ...\n",
        "######## END YOUR CODE ########\n",
        "\n",
        "# 6. Training\n",
        "# During the training process, make sure to add the training accuracy, validation accuracy, training loss, and validation loss to the list below.\n",
        "# This will allow you to visualize these metrics and assess whether the model is overfitting.\n",
        "train_acc = []\n",
        "valid_acc = []\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "\n",
        "model.train()  # Put model in training mode\n",
        "for epoch in range(epochs):\n",
        "    # TODO: Please mimic the training example of logistic regression above to write the training code here.\n",
        "    ######## YOUR CODE HERE ########\n",
        "    pass\n",
        "    ######## END YOUR CODE ########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IT4KBCESkytI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "IT4KBCESkytI",
        "outputId": "3abe148d-8c9c-442f-a3e9-e38a229b85d4"
      },
      "outputs": [],
      "source": [
        "# Plot Your Training and Validation loss in one picture here.\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(train_loss, label='Training_Loss')\n",
        "plt.plot(valid_loss, label='Validation_Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(\"Loss with epoches\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5y5aGpr8kzL_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "5y5aGpr8kzL_",
        "outputId": "b9d3275d-40cc-434d-8462-d362af030160"
      },
      "outputs": [],
      "source": [
        "# Plot Your Training and Validation accuracy in one picture here.\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(train_acc, label='Training_Acc')\n",
        "plt.plot(valid_acc, label='Validation_Acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(\"Accuracy with epoches\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RFXmDojUyQas",
      "metadata": {
        "id": "RFXmDojUyQas"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q7hkIdQqyS59",
      "metadata": {
        "id": "Q7hkIdQqyS59"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_data, device='cuda', batch_size=128):\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "    print(f'Correct/Total: {correct}/{total}')\n",
        "    print('\\nClassification Report:')\n",
        "    print(classification_report(all_labels, all_preds, target_names=test_data.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7s4COfUcy7S6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s4COfUcy7S6",
        "outputId": "ae1b2f9d-325e-4214-f7eb-f45aaa0bbdc8"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model, test_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs182_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
